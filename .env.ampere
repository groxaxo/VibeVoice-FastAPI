# =================================================================
# VibeVoice FastAPI - Ampere GPU Optimized Configuration
# =================================================================
# This configuration is optimized for NVIDIA Ampere architecture
# (RTX 3090, 3080, 3070, A100, A40, etc.)
#
# Performance Impact:
# - Real-time Factor: 0.77x (1.31x slower than real-time)
# - VRAM Usage: 10.84 GB (with INT8 quantization)
# - First Request: ~5-10s (includes torch.compile compilation)
# - Subsequent Requests: ~3-5s per 10s of audio
# =================================================================

# ============================================================
# Model Configuration
# ============================================================

# Model: VibeVoice-Large (7B parameters)
# Alternative: microsoft/VibeVoice-1.5B (1.5B, faster but lower quality)
# Alternative: microsoft/VibeVoice-Realtime-0.5B (0.5B, fastest)
VIBEVOICE_MODEL_PATH=aoi-ot/VibeVoice-Large

# Device: CUDA (required for GPU acceleration)
VIBEVOICE_DEVICE=cuda

# Optional: Specify which GPU to use (for multi-GPU systems)
# Example: CUDA_VISIBLE_DEVICES=0 (use first GPU)
# CUDA_VISIBLE_DEVICES=0

# ============================================================
# Ampere-Specific Optimizations
# ============================================================

# Data Type: bfloat16 (Native Ampere support, 2x faster than float32)
# - No conversion overhead (unlike float16)
# - Wider dynamic range than float16
# - Maintains training-level quality
VIBEVOICE_DTYPE=bfloat16

# Attention Implementation: flash_attention_2
# - 2-3x faster attention layers on Ampere
# - Memory-efficient implementation
# - Fused operations reduce kernel launch overhead
# - Utilizes Tensor Cores for mixed-precision
VIBEVOICE_ATTN_IMPLEMENTATION=flash_attention_2

# Inference Steps: 5 (Optimized for speed)
# - 5 steps: 30% faster, minor quality loss (recommended)
# - 10 steps: Balanced, default setting
# - 15 steps: High quality, slower
# - 20+ steps: Maximum quality, much slower
VIBEVOICE_INFERENCE_STEPS=5

# torch.compile: max-autotune
# - 20-50% speedup after compilation
# - Kernel fusion for better tensor core utilization
# - Automatic graph optimization for Ampere architecture
# - First request: Slower (2-5 min compilation)
# - Subsequent requests: 20-50% faster
TORCH_COMPILE=true

# torch.compile Mode: max-autotune (Fastest inference, slower compile)
# Options:
#   - max-autotune: Slowest compile, fastest inference (recommended)
#   - reduce-overhead: Faster compile, similar runtime
#   - default: Balanced compile/runtime
TORCH_COMPILE_MODE=max-autotune

# Quantization: INT8 (torchao)
# - ~40% VRAM reduction (16GB -> ~10GB)
# - Quantizes only LLM decoder to INT8
# - Audio components remain at bfloat16
# - Applied on CPU before moving to GPU
# - Minimal quality impact
VIBEVOICE_QUANTIZATION=int8_torchao

# ============================================================
# Voice Configuration
# ============================================================

# Directory containing voice preset audio files
# The API automatically loads ALL audio files from this directory
# Supported formats: .wav, .mp3, .flac, .ogg, .m4a, .aac
VOICES_DIR=./demo/voices

# OpenAI voice name to VibeVoice preset mapping (JSON format)
# Maps OpenAI-compatible voice names to your voice preset files
OPENAI_VOICE_MAPPING={"alloy": "en-Alice_woman", "echo": "en-Carter_man", "fable": "en-Maya_woman", "onyx": "en-Frank_man", "nova": "en-Mary_woman_bgm", "shimmer": "en-Alice_woman"}

# ============================================================
# API Server Configuration
# ============================================================

API_HOST=0.0.0.0
API_PORT=8001
API_WORKERS=1
API_CORS_ORIGINS=*

# ============================================================
# Generation Defaults
# ============================================================

# CFG Scale: 1.8 (Classifier-Free Guidance)
# - Range: 1.0-3.0
# - Higher = More faithful to prompt, but less diverse
# - Lower = More diverse, but less faithful
DEFAULT_CFG_SCALE=1.8

# Default audio response format
DEFAULT_RESPONSE_FORMAT=mp3

# Maximum generation length in seconds (90 minutes)
MAX_GENERATION_LENGTH=5400

# Text generation sampling (False = greedy decoding)
DEFAULT_DO_SAMPLE=false

# Sampling parameters (only used if DO_SAMPLE=True)
DEFAULT_TEMPERATURE=1.0
DEFAULT_TOP_P=1.0
DEFAULT_TOP_K=50
DEFAULT_REPETITION_PENALTY=1.0

# ============================================================
# Logging
# ============================================================

LOG_LEVEL=INFO

# ============================================================
# Performance Tuning Guide
# ============================================================

# For FASTER inference (quality trade-off):
# 1. VIBEVOICE_INFERENCE_STEPS=3 (30% faster, 10% quality loss)
# 2. Use VIBEVOICE-1.5B instead of VibeVoice-Large (2-3x faster)
# 3. TORCH_COMPILE_MODE=reduce-overhead (faster compile, similar runtime)

# For HIGHER quality (speed trade-off):
# 1. VIBEVOICE_INFERENCE_STEPS=10 or 15 (slower, better quality)
# 2. VIBEVOICE_QUANTIZATION= (disable quantization, uses ~16GB VRAM)
# 3. Use VibeVoice-Large (current model)

# For LOWER VRAM usage (if OOM):
# 1. VIBEVOICE_QUANTIZATION=int4_torchao (~60% reduction, minor quality loss)
# 2. VIBEVOICE_MODEL_PATH=microsoft/VibeVoice-1.5B (~6GB VRAM)
# 3. VIBEVOICE_INFERENCE_STEPS=3 (less memory for intermediate states)

# ============================================================
# Expected Performance (RTX 3090)
# ============================================================

# Model: VibeVoice-Large (7B)
# VRAM: 10.84 GB (INT8 quantized)
# Load Time: ~13 seconds
# Compilation: ~2-5 minutes (first request only)
# Real-time Factor: 0.77x (1.31x slower than real-time)
# 10s audio → ~13.1s to generate
# 1min audio → ~78.4s (1.3 min) to generate
# 5min audio → ~6.5 min to generate
# GPU Utilization: ~60-80% during generation
# Power Draw: ~200-250W during generation
