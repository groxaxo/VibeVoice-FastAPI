# VibeVoice FastAPI - RTX 3090 Optimized Deployment Guide

Complete guide for deploying VibeVoice TTS with maximum performance optimization on NVIDIA Ampere GPUs (RTX 3090, 3080, 3080 Ti, 3070).

## ðŸš€ Performance Overview

### Benchmark Results (RTX 3090)

| Model | Audio Duration | Generation Time | Real-time Factor | VRAM Usage |
|-------|--------------|----------------|------------------|------------|
| VibeVoice-Large (7B) | 4.13s | 5.59s | 0.74x | 10.84 GB |
| VibeVoice-Large (7B) | 8.13s | 10.28s | 0.79x | 10.84 GB |

**Average Performance: 0.77x real-time (1.31x slower than real-time)**

### Practical Generation Times

| Audio Length | Generation Time |
|-------------|----------------|
| 10 seconds | ~13.1 seconds |
| 1 minute | ~78.4 seconds (1.3 min) |
| 5 minutes | ~6.5 minutes |
| 10 minutes | ~13 minutes |

## ðŸ“‹ System Requirements

### Hardware
- **GPU**: NVIDIA Ampere architecture (RTX 3090, 3080, 3080 Ti, 3070, A100, A40)
- **VRAM**: 12 GB minimum, 16 GB recommended for 7B model
- **RAM**: 16 GB minimum, 32 GB recommended
- **Storage**: 50 GB minimum (for model cache)

### Software
- **OS**: Linux (Ubuntu 20.04+ recommended)
- **CUDA**: 12.0+ (12.8 recommended)
- **Python**: 3.10-3.11 (3.11 recommended)
- **Docker**: 24.0+ (optional, for containerized deployment)

## ðŸ“¦ Complete Dependencies

### Core Dependencies

```bash
# Python packages
torch==2.8.0+cu128
torchvision==0.23.0+cu128
torchaudio==2.8.0+cu128
flash-attn==2.8.3
torchao==0.13.0

# VibeVoice core
accelerate==1.6.0
transformers==4.51.3
diffusers>=0.36.0
huggingface-hub>=0.36.0
safetensors>=0.7.0

# Audio processing
librosa==0.11.0
soundfile>=0.12.1
pydub>=0.25.1
av>=16.1.0

# API framework
fastapi>=0.128.5
uvicorn[standard]>=0.40.0
python-multipart>=0.0.22
pydantic>=2.12.0
pydantic-settings>=2.12.0
python-dotenv>=1.0.0
sse-starlette>=3.2.0
httpx>=0.26.0
aiofiles>=24.1.0

# Utilities
numpy>=2.3.0
scipy>=1.17.0
tqdm>=4.67.0
ml-collections>=1.1.0
absl-py>=2.4.0
gradio>=6.5.1
aiortc>=1.14.0
```

### System Dependencies

```bash
# Ubuntu/Debian
apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    git \
    ffmpeg \
    libsndfile1 \
    curl \
    wget \
    build-essential \
    cuda-toolkit-12-8 \
    nvidia-container-toolkit

# NVIDIA drivers (if not installed)
apt-get install -y nvidia-driver-570
```

### CUDA Dependencies

```bash
# NVIDIA CUDA libraries (installed with PyTorch wheel)
nvidia-cuda-runtime-cu12==12.8.90
nvidia-cuda-cupti-cu12==12.8.90
nvidia-cudnn-cu12==9.10.2.21
nvidia-cublas-cu12==12.8.4.1
nvidia-cufft-cu12==11.3.3.83
nvidia-curand-cu12==10.3.9.90
nvidia-cusolver-cu12==11.7.3.90
nvidia-cusparse-cu12==12.5.8.93
nvidia-cusparselt-cu12==0.7.1
nvidia-nccl-cu12==2.27.3
nvidia-nvjitlink-cu12==12.8.93
nvidia-cufile-cu12==1.13.1.3
triton==3.4.0
```

## ðŸ”§ Optimizations for Ampere GPUs

### 1. INT8 Quantization (torchao)

**Impact**: ~40% VRAM reduction, minimal quality loss

```python
# Enabled via environment variable
VIBEVOICE_QUANTIZATION=int8_torchao
```

**Implementation Details**:
- Quantizes only the LLM decoder (Qwen2) to INT8
- Audio components remain at full precision (bfloat16)
- Applied on CPU before moving to GPU (saves VRAM during loading)

**VRAM Savings**:
- Without quantization: ~16-18 GB
- With INT8 quantization: ~10-11 GB

### 2. Flash Attention 2

**Impact**: 2-3x speedup on attention layers

```python
VIBEVOICE_ATTN_IMPLEMENTATION=flash_attention_2
```

**Ampere-Specific Benefits**:
- Memory-efficient attention implementation
- Fused operations reduce kernel launch overhead
- Tensor Core utilization for mixed-precision

**Installation**:
```bash
pip install flash-attn==2.8.3+cu12torch2.8cxx11abiFALSE
```

### 3. torch.compile (max-autotune)

**Impact**: 20-50% speedup after compilation

```python
TORCH_COMPILE=true
TORCH_COMPILE_MODE=max-autotune
```

**Ampere-Specific Optimizations**:
- Kernel fusion for better tensor core utilization
- Automatic graph optimization for Ampere architecture
- Just-In-Time (JIT) compilation with CUDA graphs

**Trade-offs**:
- First request: Slower (compilation time ~2-5 min)
- Subsequent requests: 20-50% faster
- More VRAM usage for compiled kernels

### 4. bfloat16 Precision

**Impact**: 2x faster than float32, minimal quality loss

```python
VIBEVOICE_DTYPE=bfloat16
```

**Ampere-Specific Benefits**:
- Native bfloat16 support on Ampere GPUs
- No conversion overhead (unlike float16)
- Wider dynamic range than float16
- Maintains training-level quality

### 5. Reduced Inference Steps

**Impact**: 30% faster per step reduction

```python
VIBEVOICE_INFERENCE_STEPS=5
```

**Configuration Options**:
- **5 steps**: Fastest, minor quality loss (recommended for RTF > 1x)
- **10 steps**: Balanced (default)
- **15 steps**: High quality, slower
- **20+ steps**: Maximum quality, much slower

**Quality Impact**:
- 5 vs 10 steps: ~10% quality reduction (hardly noticeable)
- 5 vs 20 steps: ~20-25% quality reduction (noticeable)

## ðŸ³ Docker Deployment

### Optimized Dockerfile for Ampere GPUs

```dockerfile
# Base image with CUDA 12.8
FROM nvidia/cuda:12.8.1-cudnn-runtime-ubuntu24.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_NO_BUILD_ISOLATION=0 \
    PIP_ONLY_BINARY=:all: \
    MAX_JOBS=2 \
    MAKEFLAGS="-j2"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    git \
    ffmpeg \
    libsndfile1 \
    curl \
    wget \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Set working directory
WORKDIR /app

# Create virtual environment
RUN python3 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Upgrade pip
RUN pip install --upgrade pip setuptools wheel

# Copy requirements first for better caching
COPY pyproject.toml README.md ./
COPY requirements-api.txt ./
COPY vibevoice/ ./vibevoice/
COPY demo/ ./demo/
COPY api/ ./api/

# Install PyTorch with CUDA support (CUDA 12.8 compatible)
RUN pip install torch==2.8.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Install torchao for INT8 quantization support (PyTorch 2.8 compatible)
RUN pip install torchao==0.13.0

# Install flash-attn from pre-built wheel (Ampere optimized)
RUN pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp311-cp311-linux_x86_64.whl || \
    echo "WARNING: flash-attn wheel install failed, continuing without it"

# Install VibeVoice package
RUN pip install --only-binary=:all: -e . || pip install -e .

# Install API dependencies
RUN pip install --only-binary=:all: -r requirements-api.txt || pip install -r requirements-api.txt

# Create directories for voices and models
RUN mkdir -p /app/voices /app/models

# Copy configuration
COPY .env ./

# Expose API port
EXPOSE 8001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

# Run the API
CMD ["sh", "-c", "/app/venv/bin/uvicorn api.main:app --host 0.0.0.0 --port ${API_PORT:-8001}"]
```

### Docker Compose Configuration

```yaml
version: '3.8'

services:
  vibevoice:
    build: .
    image: vibevoice-ampere:latest
    container_name: vibevoice-server
    ports:
      - "8001:8001"
    volumes:
      - ./demo/voices:/app/voices:ro
      - ./vibevoice.log:/app/vibevoice.log
    environment:
      - VIBEVOICE_MODEL_PATH=aoi-ot/VibeVoice-Large
      - VIBEVOICE_DEVICE=cuda
      - VIBEVOICE_DTYPE=bfloat16
      - VIBEVOICE_ATTN_IMPLEMENTATION=flash_attention_2
      - VIBEVOICE_INFERENCE_STEPS=5
      - TORCH_COMPILE=true
      - TORCH_COMPILE_MODE=max-autotune
      - VIBEVOICE_QUANTIZATION=int8_torchao
      - API_PORT=8001
      - VOICES_DIR=/app/voices
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

### Build and Run

```bash
# Build image
docker-compose build

# Run in detached mode
docker-compose up -d

# View logs
docker-compose logs -f

# Stop service
docker-compose down
```

## ðŸš€ Quick Start (Conda)

### 1. Create Environment

```bash
# Create conda environment with Python 3.11
conda create -n vibevoice python=3.11 -y
conda activate vibevoice
```

### 2. Install Dependencies

```bash
# Upgrade pip
pip install --upgrade pip setuptools wheel

# Install PyTorch with CUDA 12.8
pip install torch==2.8.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Install flash-attn (pre-built wheel for Ampere)
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

# Install torchao
pip install torchao==0.13.0

# Install VibeVoice
pip install -e .

# Install API dependencies
pip install -r requirements-api.txt
```

### 3. Configure Environment

```bash
# Copy and edit .env
cp env.example .env
```

Edit `.env` with Ampere-optimized settings:

```bash
# Model Configuration
VIBEVOICE_MODEL_PATH=aoi-ot/VibeVoice-Large
VIBEVOICE_DEVICE=cuda
VIBEVOICE_DTYPE=bfloat16
VIBEVOICE_ATTN_IMPLEMENTATION=flash_attention_2
VIBEVOICE_INFERENCE_STEPS=5
TORCH_COMPILE=true
TORCH_COMPILE_MODE=max-autotune
VIBEVOICE_QUANTIZATION=int8_torchao

# API Configuration
API_HOST=0.0.0.0
API_PORT=8001
VOICES_DIR=./demo/voices

# Generation Defaults
DEFAULT_CFG_SCALE=1.8
DEFAULT_RESPONSE_FORMAT=mp3
```

### 4. Deploy with nohup

```bash
# Activate conda environment
conda activate vibevoice

# Start server in background
nohup uvicorn api.main:app --host 0.0.0.0 --port 8001 > vibevoice.log 2>&1 &
echo $! > vibevoice.pid

# Monitor logs
tail -f vibevoice.log
```

### 5. Test Deployment

```bash
# Health check
curl http://localhost:8001/health

# List voices
curl http://localhost:8001/v1/audio/voices

# Generate speech
curl -X POST http://localhost:8001/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tts-1",
    "input": "Speaker 0: Hello, this is a test.",
    "voice": "alloy",
    "response_format": "mp3"
  }' \
  --output test.mp3
```

## âš¡ Performance Tuning Guide

### Speed vs Quality Trade-offs

| Configuration | Speed | Quality | Use Case |
|--------------|--------|---------|-----------|
| 3 steps, no compile | 3x fastest | 70% | Real-time apps |
| 5 steps, max-autotune | 2x faster | 85% | Production (current) |
| 10 steps, max-autotune | Baseline | 95% | High-quality |
| 20 steps, no compile | 2x slower | 100% | Maximum quality |

### VRAM Optimization

| Strategy | VRAM Usage | Speed Impact | Quality Impact |
|----------|------------|-------------|----------------|
| INT8 quantization | -40% | Neutral | None |
| 4-bit quantization | -60% | +10-20% faster | Minor loss |
| CPU offload | -80% | 5-10x slower | None |

### Multi-GPU Configuration

For multi-GPU systems (e.g., 3x RTX 3090):

```bash
# Use specific GPU
CUDA_VISIBLE_DEVICES=0 uvicorn api.main:app --port 8001
CUDA_VISIBLE_DEVICES=1 uvicorn api.main:app --port 8002
CUDA_VISIBLE_DEVICES=2 uvicorn api.main:app --port 8003

# Or use load balancer
```

## ðŸ“Š Performance Monitoring

### GPU Usage

```bash
# Monitor GPU utilization
watch -n 1 nvidia-smi

# Detailed metrics
nvidia-smi dmon -s puct -d 5

# Temperature and power
nvidia-smi --query-gpu=temperature.gpu,power.draw,utilization.gpu --format=csv -l 1
```

### API Performance

```bash
# Monitor logs for generation times
tail -f vibevoice.log | grep "Generation Time"

# Test latency
time curl -X POST http://localhost:8001/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model":"tts-1","input":"Test","voice":"alloy"}'
```

## ðŸ” Troubleshooting

### Out of Memory

```bash
# Solution 1: Reduce inference steps
VIBEVOICE_INFERENCE_STEPS=3

# Solution 2: Enable INT8 quantization
VIBEVOICE_QUANTIZATION=int8_torchao

# Solution 3: Use smaller model
VIBEVOICE_MODEL_PATH=microsoft/VibeVoice-1.5B
```

### Slow Generation

```bash
# Solution 1: Check flash-attn is working
python -c "import flash_attn; print(flash_attn.__version__)"

# Solution 2: Verify torch.compile is enabled
grep TORCH_COMPILE .env

# Solution 3: Reduce inference steps
VIBEVOICE_INFERENCE_STEPS=5
```

### torch.compile Issues

```bash
# Disable if compilation fails
TORCH_COMPILE=false

# Or use reduce-overhead mode (faster compile, similar runtime)
TORCH_COMPILE_MODE=reduce-overhead
```

## ðŸ“ API Usage Examples

### OpenAI-Compatible API

```bash
# Generate speech
curl -X POST http://localhost:8001/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tts-1",
    "input": "Speaker 0: Your text here",
    "voice": "alloy",
    "response_format": "mp3"
  }' \
  --output speech.mp3
```

### Python Client

```python
import requests

response = requests.post(
    "http://localhost:8001/v1/audio/speech",
    json={
        "model": "tts-1",
        "input": "Speaker 0: Your text here",
        "voice": "alloy",
        "response_format": "mp3"
    }
)

with open("speech.mp3", "wb") as f:
    f.write(response.content)
```

## ðŸ”§ Maintenance

### Stop Server

```bash
# Using PID file
kill $(cat vibevoice.pid)
rm vibevoice.pid

# Or find process
pkill -f "uvicorn api.main:app"
```

### Update Model

```bash
# Change model in .env
sed -i 's/VIBEVOICE_MODEL_PATH=.*/VIBEVOICE_MODEL_PATH=aoi-ot\/VibeVoice-Large/' .env

# Restart server
kill $(cat vibevoice.pid) && rm vibevoice.pid
nohup uvicorn api.main:app --host 0.0.0.0 --port 8001 > vibevoice.log 2>&1 &
echo $! > vibevoice.pid
```

### Clean Cache

```bash
# Clear HuggingFace cache
rm -rf ~/.cache/huggingface/hub/

# Clear PyTorch cache
python -c "import torch; torch.cuda.empty_cache()"
```

## ðŸ“š Additional Resources

- [VibeVoice Documentation](https://github.com/microsoft/VibeVoice)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [torchao](https://github.com/pytorch/ao)
- [Ampere Architecture](https://www.nvidia.com/en-us/data-center/ampere-architecture/)

## ðŸ“„ License

This deployment guide maintains the original VibeVoice model license. Please refer to the original VibeVoice license for model usage terms.

---

**Last Updated**: 2026-02-09  
**Tested Hardware**: NVIDIA GeForce RTX 3090 x3  
**Tested Software**: CUDA 12.8, PyTorch 2.8.0, Python 3.11
